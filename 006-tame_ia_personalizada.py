#!/usr/bin/env python3
"""
====================================================
  MÃ“DULO 6 - IA PERSONALIZADA TAME (Ollama local)
====================================================
TAME (Tutor AutÃ³nomo de MÃ³dulos de Estudio) es una
IA personalizada que actÃºa como asistente docente
para la asignatura de ProgramaciÃ³n de Servicios y
Procesos de DAM-2.

Usa el modelo local de Ollama con un system prompt
personalizado que define la identidad de TAME.

Requiere: pip install requests
          ollama corriendo en localhost:11434
====================================================
"""

import json
import requests

OLLAMA_URL = "http://localhost:11434/api/chat"
MODELO     = "qwen2.5:7b-instruct-q4_0"   # Modelo por defecto del aula

# Historial de la conversaciÃ³n (memoria de contexto)
HISTORIAL: list[dict] = []

# Personalidad de TAME
SYSTEM_PROMPT = """Eres TAME, el Tutor AutÃ³nomo de MÃ³dulos de Estudio.
Eres un asistente docente especializado en la asignatura de
'ProgramaciÃ³n de Servicios y Procesos' del ciclo DAM-2 (Desarrollo de
Aplicaciones Multiplataforma).

Tu especialidad cubre los siguientes bloques temÃ¡ticos:
1. ProgramaciÃ³n multiproceso (subprocess, multiprocessing, psutil)
2. ProgramaciÃ³n multihilo (threading, concurrent.futures)
3. Comunicaciones en red (sockets TCP/UDP, SMTP, IMAP)
4. GeneraciÃ³n de servicios en red (WebSockets, APIs REST, HTTP)
5. ProgramaciÃ³n segura (gestiÃ³n de errores, cifrado bÃ¡sico, .env)

Responde siempre en espaÃ±ol, de forma clara y pedagÃ³gica. Si el alumno
tiene dudas de cÃ³digo, muestra ejemplos concretos en Python. Cuando
expliques conceptos tÃ©cnicos, usa analogÃ­as sencillas. Eres amable,
paciente y motivador. Si alguien te pregunta quiÃ©n eres, di que eres
TAME y explica brevemente tu funciÃ³n."""


def chat_tame(pregunta: str) -> str:
    """
    EnvÃ­a una pregunta a TAME (Ollama) manteniendo el historial
    de conversaciÃ³n, y retorna la respuesta como texto.
    """
    # AÃ±adir la pregunta del usuario al historial
    HISTORIAL.append({"role": "user", "content": pregunta})

    payload = {
        "model":    MODELO,
        "messages": [{"role": "system", "content": SYSTEM_PROMPT}] + HISTORIAL,
        "stream":   True,
    }

    try:
        respuesta_http = requests.post(
            OLLAMA_URL,
            json=payload,
            timeout=120,
            stream=True,
        )
        respuesta_http.raise_for_status()
    except requests.exceptions.ConnectionError:
        HISTORIAL.pop()   # Deshacer el mensaje no respondido
        return "âŒ No se pudo conectar a Ollama. Â¿EstÃ¡ en ejecuciÃ³n en localhost:11434?"
    except requests.exceptions.RequestException as e:
        HISTORIAL.pop()
        return f"âŒ Error en la solicitud a Ollama: {e}"

    # Acumular la respuesta en streaming
    texto_respuesta = ""
    print("[TAME] ", end="", flush=True)

    for linea in respuesta_http.iter_lines():
        if not linea:
            continue
        try:
            chunk = json.loads(linea.decode("utf-8"))
        except json.JSONDecodeError:
            continue

        delta = chunk.get("message", {}).get("content", "")
        if delta:
            print(delta, end="", flush=True)
            texto_respuesta += delta

        if chunk.get("done", False):
            break

    print()  # Nueva lÃ­nea al terminar el streaming

    # AÃ±adir respuesta del asistente al historial
    HISTORIAL.append({"role": "assistant", "content": texto_respuesta})

    return texto_respuesta


def main():
    print("=" * 60)
    print("  TAME â€” Tutor AutÃ³nomo de MÃ³dulos de Estudio")
    print("  Asignatura: ProgramaciÃ³n de Servicios y Procesos")
    print("  Modelo: " + MODELO)
    print("=" * 60)
    print("  Escribe tu pregunta o 'salir' para terminar.")
    print("  Escribe 'limpiar' para borrar el historial de conversaciÃ³n.\n")

    while True:
        try:
            pregunta = input("Alumno > ").strip()
        except (EOFError, KeyboardInterrupt):
            print("\n[TAME] ðŸ‘‹ Â¡Hasta la prÃ³xima! Sigue practicando.")
            break

        if not pregunta:
            continue
        if pregunta.lower() in ("salir", "exit", "quit"):
            print("[TAME] ðŸ‘‹ Â¡Hasta la prÃ³xima! Recuerda repasar los apuntes. ðŸ˜Š")
            break
        if pregunta.lower() in ("limpiar", "reset", "nueva"):
            HISTORIAL.clear()
            print("[TAME] âœ… Historial borrado. Nueva conversaciÃ³n.\n")
            continue

        chat_tame(pregunta)
        print()


if __name__ == "__main__":
    main()
