#!/usr/bin/env python3
"""
====================================================
  MÃ“DULO 7 - CONEXIÃ“N A IA CON PYTHON (Ollama API)
====================================================
Demuestra tres formas distintas de conectarse a un
servidor de IA con Python:

  A) Llamada directa a la API REST de Ollama (/api/generate)
     con requests y sin streaming (respuesta Ãºnica).

  B) Llamada a Ollama con streaming lÃ­nea a lÃ­nea,
     acumulando el texto completo.

  C) Uso del paquete oficial 'ollama' de Python.

Requiere: pip install requests ollama
          ollama corriendo en localhost:11434
====================================================
"""

import json
import sys
import requests

# IntegraciÃ³n con base de datos
try:
    from database_models import SessionLocal, crear_log
    DB_DISPONIBLE = True
except ImportError:
    DB_DISPONIBLE = False

OLLAMA_BASE   = "http://localhost:11434"
MODELO        = "qwen2.5:7b-instruct-q4_0"


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# FORMA A: /api/generate  sin streaming
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def generar_sin_streaming(prompt: str) -> str:
    """
    Llama al endpoint /api/generate de Ollama con stream:false.
    Espera a recibir toda la respuesta de una vez.
    """
    # Registrar consulta
    if DB_DISPONIBLE:
        db = SessionLocal()
        try:
            crear_log(db, "INFO", "Ollama", f"Consulta (sin stream): {prompt[:50]}...")
        finally:
            db.close()
    
    url = f"{OLLAMA_BASE}/api/generate"
    payload = {
        "model":  MODELO,
        "prompt": prompt,
        "stream": False,
    }
    try:
        r = requests.post(url, json=payload, timeout=180)
        r.raise_for_status()
        datos = r.json()
        return datos.get("response", "(Sin respuesta)")
    except requests.exceptions.ConnectionError:
        error_msg = "âŒ Ollama no estÃ¡ en ejecuciÃ³n (localhost:11434)."
        if DB_DISPONIBLE:
            db = SessionLocal()
            try:
                crear_log(db, "ERROR", "Ollama", "Servicio no disponible")
            finally:
                db.close()
        return error_msg
    except requests.exceptions.RequestException as e:
        if DB_DISPONIBLE:
            db = SessionLocal()
            try:
                crear_log(db, "ERROR", "Ollama", str(e))
            finally:
                db.close()
        return f"âŒ Error: {e}"


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# FORMA B: /api/generate  CON streaming  manual
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def generar_con_streaming(prompt: str) -> str:
    """
    Llama al endpoint /api/generate de Ollama con stream:true.
    Imprime cada fragmento en tiempo real y retorna el texto completo.
    """
    url = f"{OLLAMA_BASE}/api/generate"
    payload = {
        "model":  MODELO,
        "prompt": prompt,
        "stream": True,
    }
    texto_completo = ""
    try:
        with requests.post(url, json=payload, timeout=180, stream=True) as r:
            r.raise_for_status()
            for linea in r.iter_lines():
                if not linea:
                    continue
                chunk = json.loads(linea.decode("utf-8"))
                fragmento = chunk.get("response", "")
                if fragmento:
                    print(fragmento, end="", flush=True)
                    texto_completo += fragmento
                if chunk.get("done", False):
                    break
        print()
        return texto_completo
    except requests.exceptions.ConnectionError:
        return "\nâŒ Ollama no estÃ¡ en ejecuciÃ³n (localhost:11434)."
    except requests.exceptions.RequestException as e:
        return f"\nâŒ Error: {e}"


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# FORMA C: paquete oficial 'ollama'
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def generar_con_libreria_ollama(prompt: str) -> str:
    """
    Usa el paquete oficial 'ollama' para Python.
    MÃ¡s limpio y moderno que llamar a la API manualmente.
    """
    try:
        import ollama as ollama_pkg
    except ImportError:
        return "âŒ El paquete 'ollama' no estÃ¡ instalado. Ejecuta: pip install ollama"

    try:
        respuesta = ollama_pkg.generate(model=MODELO, prompt=prompt)
        return respuesta["response"]
    except Exception as e:
        return f"âŒ Error con la librerÃ­a ollama: {e}"


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# PROGRAMA PRINCIPAL â€” MenÃº de demostraciÃ³n
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def mostrar_menu() -> None:
    print("\n  Elige el mÃ©todo de conexiÃ³n:")
    print("  [A] /api/generate  sin streaming (respuesta Ãºnica al final)")
    print("  [B] /api/generate  CON streaming  (texto en tiempo real)")
    print("  [C] Paquete oficial 'ollama'")
    print("  [S] Salir")


def main():
    print("=" * 60)
    print("  CONEXIÃ“N A IA CON PYTHON â€” Ollama API")
    print(f"  Servidor: {OLLAMA_BASE}")
    print(f"  Modelo  : {MODELO}")
    print("=" * 60)

    while True:
        mostrar_menu()
        try:
            opcion = input("\nOpciÃ³n > ").strip().upper()
        except (EOFError, KeyboardInterrupt):
            print("\nðŸ‘‹ Saliendo.")
            break

        if opcion == "S":
            print("ðŸ‘‹ Â¡Hasta pronto!")
            break

        if opcion not in ("A", "B", "C"):
            print("âš ï¸  OpciÃ³n no vÃ¡lida.")
            continue

        try:
            prompt = input("Prompt > ").strip()
        except (EOFError, KeyboardInterrupt):
            break

        if not prompt:
            continue

        print("\n" + "â”€" * 60)

        if opcion == "A":
            print("[A] Respuesta sin streaming:\n")
            respuesta = generar_sin_streaming(prompt)
            print(respuesta)

        elif opcion == "B":
            print("[B] Respuesta con streaming:\n")
            generar_con_streaming(prompt)

        elif opcion == "C":
            print("[C] Respuesta con librerÃ­a 'ollama':\n")
            respuesta = generar_con_libreria_ollama(prompt)
            print(respuesta)

        print("â”€" * 60)


if __name__ == "__main__":
    main()
